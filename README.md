# MSCI 641 Project 

This repository contains the code and data for Comprehensive Evaluation of Transformer-Based Models for Abstractive News Article Summarization Using the CNN/DailyMail Dataset.

## Table of Contents
1. [Introduction](#introduction)
2. [Models Evaluated](#models-evaluated)
3. [Dataset](#dataset)
4. [Models](#model)

## Introduction
This project aims to evaluate various transformer-based models for the task of abstractive news article summarization using the CNN/DailyMail dataset. The models assessed include PEGASUS, BART, T5, RoBERTa, and a Seq2Seq model.

## Models Evaluated
- **PEGASUS**: Known for its gap-sentence generation pretraining objective.
- **BART**: Combines the strengths of BERT and GPT with a bidirectional and autoregressive transformer architecture.
- **T5**: Reframes all NLP tasks into a text-to-text format.
- **RoBERTa**: Primarily designed for understanding tasks, adapted for summarization.
- **Seq2Seq**: A model trained from scratch for comparison purposes.

## Dataset
The CNN/DailyMail dataset is used for training and evaluation - [Link](https://www.kaggle.com/datasets/gowrishankarp/newspaper-text-summarization-cnn-dailymail/data). 

The CNN/DailyMail dataset on Kaggle is designed for text summarization tasks. It comprises over 300,000 news articles from CNN and the Daily Mail, along with their corresponding summaries. This dataset is widely used in the development and evaluation of machine learning models for summarization, providing a rich resource for NLP research.

## Model

The finetuned models are available here - [Link]
